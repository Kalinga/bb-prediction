{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras_lstm.py\n",
    "    \n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import numpy as np\n",
    "\n",
    "# for scaling and inverse_transform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "\n",
    "import pickle\n",
    "from os import listdir\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tprint(type(data))\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1): # range([start,] stop [, step])\n",
    "        # DataFrame.shift(periods=1, freq=None, axis=0), returns DataFrame\n",
    "\t\tcols.append(df.shift(i)) \n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\t# We are only interested in final bb after certain frames, e.g. 31st, 61st etc\n",
    "\tfor i in range(n_out - 1 , n_out): #(n_out th element from the last element in the sequence)\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "    #  concatenating along the columns (axis=1), a DataFrame is returned.\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1900.  953. 1919. 1079.]\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"min-max-scaler.pkl\",\"rb\")\n",
    "scaler = pickle.load(pickle_in)\n",
    "print(scaler.data_max_)\n",
    "print(scaler.data_min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84371, 4)\n",
      "[1900.  953. 1919. 1079.]\n",
      "[0. 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "(84312, 124)\n",
      "(64372, 120) 64372 (64372, 4)\n",
      "(64372, 30, 4) (64372, 4) (19940, 30, 4) (19940, 4)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('bb-cross-train.csv', header=0)\n",
    "values = dataset.values\n",
    "print(values.shape)\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "print(scaler.data_max_)\n",
    "print(scaler.data_min_)\n",
    "\n",
    "pickle.dump(scaler, open(\"min-max-scaler.pkl\", 'wb'))\n",
    "\n",
    "# specify the number of lag hours\n",
    "n_seq = 30\n",
    "n_seq_future = 30\n",
    "n_features = 4\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_seq, n_seq_future)\n",
    "print(reframed.shape)\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_num = 84372 - 20000\n",
    "train = values[:n_train_num, :]\n",
    "val = values[n_train_num:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_seq * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, n_obs:n_obs+n_features]\n",
    "val_X, val_y = val[:, :n_obs], val[:, n_obs:n_obs+n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_seq, n_features))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_seq, n_features))\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = [\"train/%s\" %f for f in listdir(\"./train\")]\n",
    "#print(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del reframed\n",
    "reframed = np.zeros( (1,124) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reframed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(226, 124)\n",
      "(84, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(251, 124)\n",
      "(273, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(465, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(525, 124)\n",
      "(170, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(636, 124)\n",
      "(113, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(690, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(870, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(960, 124)\n",
      "(330, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1231, 124)\n",
      "(144, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1316, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1406, 124)\n",
      "(289, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1636, 124)\n",
      "(293, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1870, 124)\n",
      "(175, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1986, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2166, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2286, 124)\n",
      "(299, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2526, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2706, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2796, 124)\n",
      "(107, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2844, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(2994, 124)\n",
      "(237, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(3172, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(3322, 124)\n",
      "(201, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(3464, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(3644, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(3794, 124)\n",
      "(414, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4149, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4299, 124)\n",
      "(200, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4440, 124)\n",
      "(358, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4739, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4859, 124)\n",
      "(130, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4930, 124)\n",
      "(89, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(4960, 124)\n",
      "(216, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(5117, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(5327, 124)\n",
      "(167, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(5435, 124)\n",
      "(321, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(5697, 124)\n",
      "(289, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(5927, 124)\n",
      "(299, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6167, 124)\n",
      "(232, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6340, 124)\n",
      "(214, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6495, 124)\n",
      "(256, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6692, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6782, 124)\n",
      "(108, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6831, 124)\n",
      "(84, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6856, 124)\n",
      "(89, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(6886, 124)\n",
      "(250, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7077, 124)\n",
      "(185, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7203, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7383, 124)\n",
      "(74, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7398, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7578, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7758, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(7968, 124)\n",
      "(268, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(8177, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(8387, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(8507, 124)\n",
      "(160, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(8608, 124)\n",
      "(253, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(8802, 124)\n",
      "(197, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(8940, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9030, 124)\n",
      "(339, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9310, 124)\n",
      "(45, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9310, 124)\n",
      "(122, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9373, 124)\n",
      "(296, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9610, 124)\n",
      "(180, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9731, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(9941, 124)\n",
      "(327, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(10209, 124)\n",
      "(259, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(10409, 124)\n",
      "(167, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(10517, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(10697, 124)\n",
      "(262, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(10900, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11080, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11140, 124)\n",
      "(292, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11373, 124)\n",
      "(224, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11538, 124)\n",
      "(39, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11538, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11598, 124)\n",
      "(135, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11674, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11884, 124)\n",
      "(174, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(11999, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12149, 124)\n",
      "(263, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12353, 124)\n",
      "(31, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12353, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12473, 124)\n",
      "(157, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12571, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12691, 124)\n",
      "(286, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(12918, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13068, 124)\n",
      "(168, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13177, 124)\n",
      "(121, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13239, 124)\n",
      "(199, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13379, 124)\n",
      "(228, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13548, 124)\n",
      "(380, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13869, 124)\n",
      "(174, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(13984, 124)\n",
      "(280, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(14205, 124)\n",
      "(350, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(14496, 124)\n",
      "(329, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(14766, 124)\n",
      "(277, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(14984, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15074, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15194, 124)\n",
      "(224, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15359, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15449, 124)\n",
      "(264, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15654, 124)\n",
      "(176, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15771, 124)\n",
      "(222, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(15934, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16114, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16204, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16354, 124)\n",
      "(204, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16499, 124)\n",
      "(150, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16590, 124)\n",
      "(281, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16812, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(16992, 124)\n",
      "(328, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(17261, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(17471, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(17561, 124)\n",
      "(161, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(17663, 124)\n",
      "(169, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(17773, 124)\n",
      "(232, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(17946, 124)\n",
      "(288, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18175, 124)\n",
      "(104, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18220, 124)\n",
      "(259, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18420, 124)\n",
      "(299, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18660, 124)\n",
      "(231, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18832, 124)\n",
      "(39, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18832, 124)\n",
      "(224, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(18997, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19057, 124)\n",
      "(151, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19149, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19209, 124)\n",
      "(312, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19462, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19672, 124)\n",
      "(202, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19815, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(19875, 124)\n",
      "(272, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(20088, 124)\n",
      "(232, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(20261, 124)\n",
      "(586, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(20788, 124)\n",
      "(175, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(20904, 124)\n",
      "(204, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21049, 124)\n",
      "(151, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21141, 124)\n",
      "(260, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21342, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21432, 124)\n",
      "(431, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21804, 124)\n",
      "(155, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21900, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(21990, 124)\n",
      "(111, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(22042, 124)\n",
      "(329, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(22312, 124)\n",
      "(237, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(22490, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(22550, 124)\n",
      "(359, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(22850, 124)\n",
      "(469, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23260, 124)\n",
      "(244, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23445, 124)\n",
      "(150, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23536, 124)\n",
      "(90, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23567, 124)\n",
      "(171, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23679, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23859, 124)\n",
      "(108, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(23908, 124)\n",
      "(166, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24015, 124)\n",
      "(134, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24090, 124)\n",
      "(137, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24168, 124)\n",
      "(230, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24339, 124)\n",
      "(230, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24510, 124)\n",
      "(319, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24770, 124)\n",
      "(252, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(24963, 124)\n",
      "(244, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(25148, 124)\n",
      "(296, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(25385, 124)\n",
      "(276, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(25602, 124)\n",
      "(218, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(25761, 124)\n",
      "(49, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(25761, 124)\n",
      "(208, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(25910, 124)\n",
      "(311, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26162, 124)\n",
      "(188, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26291, 124)\n",
      "(203, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26435, 124)\n",
      "(281, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26657, 124)\n",
      "(115, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26713, 124)\n",
      "(201, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26855, 124)\n",
      "(71, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(26867, 124)\n",
      "(233, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27041, 124)\n",
      "(61, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27043, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27223, 124)\n",
      "(109, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27273, 124)\n",
      "(203, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27417, 124)\n",
      "(329, 4)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27687, 124)\n",
      "(172, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27800, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27890, 124)\n",
      "(160, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(27991, 124)\n",
      "(174, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28106, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28196, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28256, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28346, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28436, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28616, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28706, 124)\n",
      "(281, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(28928, 124)\n",
      "(206, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29075, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29225, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29345, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29555, 124)\n",
      "(231, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29727, 124)\n",
      "(182, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29850, 124)\n",
      "(118, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29909, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(29999, 124)\n",
      "(242, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(30182, 124)\n",
      "(495, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(30618, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(30798, 124)\n",
      "(169, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(30908, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(30998, 124)\n",
      "(296, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(31235, 124)\n",
      "(334, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(31510, 124)\n",
      "(329, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(31780, 124)\n",
      "(241, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(31962, 124)\n",
      "(293, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(32196, 124)\n",
      "(269, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(32406, 124)\n",
      "(177, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(32524, 124)\n",
      "(142, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(32607, 124)\n",
      "(222, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(32770, 124)\n",
      "(201, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(32912, 124)\n",
      "(234, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(33087, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(33267, 124)\n",
      "(255, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(33463, 124)\n",
      "(194, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(33598, 124)\n",
      "(114, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(33653, 124)\n",
      "(323, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(33917, 124)\n",
      "(225, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34083, 124)\n",
      "(389, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34413, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34503, 124)\n",
      "(232, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34676, 124)\n",
      "(116, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34733, 124)\n",
      "(76, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34750, 124)\n",
      "(129, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34820, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(34940, 124)\n",
      "(260, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35141, 124)\n",
      "(84, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35166, 124)\n",
      "(290, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35397, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35487, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35547, 124)\n",
      "(134, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35622, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35772, 124)\n",
      "(72, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35785, 124)\n",
      "(215, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(35941, 124)\n",
      "(204, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36086, 124)\n",
      "(115, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36142, 124)\n",
      "(241, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36324, 124)\n",
      "(197, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36462, 124)\n",
      "(172, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36575, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36635, 124)\n",
      "(117, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36693, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36843, 124)\n",
      "(188, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(36972, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37152, 124)\n",
      "(160, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37253, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37403, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37523, 124)\n",
      "(199, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37663, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37723, 124)\n",
      "(89, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37753, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(37873, 124)\n",
      "(210, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38024, 124)\n",
      "(243, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38208, 124)\n",
      "(312, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38461, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38581, 124)\n",
      "(105, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38627, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38777, 124)\n",
      "(125, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38843, 124)\n",
      "(141, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(38925, 124)\n",
      "(266, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39132, 124)\n",
      "(227, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39300, 124)\n",
      "(111, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39352, 124)\n",
      "(103, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39396, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39486, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39576, 124)\n",
      "(359, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39876, 124)\n",
      "(109, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(39926, 124)\n",
      "(232, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40099, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40249, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40339, 124)\n",
      "(164, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40444, 124)\n",
      "(89, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40474, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40564, 124)\n",
      "(137, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40642, 124)\n",
      "(170, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(40753, 124)\n",
      "(309, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41003, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41183, 124)\n",
      "(259, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41383, 124)\n",
      "(231, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41555, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41615, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41765, 124)\n",
      "(290, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(41996, 124)\n",
      "(369, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(42306, 124)\n",
      "(193, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(42440, 124)\n",
      "(230, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(42611, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(42701, 124)\n",
      "(247, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(42889, 124)\n",
      "(259, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(43089, 124)\n",
      "(109, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(43139, 124)\n",
      "(218, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(43298, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(43478, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(43568, 124)\n",
      "(579, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44088, 124)\n",
      "(287, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44316, 124)\n",
      "(153, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44410, 124)\n",
      "(143, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44494, 124)\n",
      "(151, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44586, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44646, 124)\n",
      "(368, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(44955, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45045, 124)\n",
      "(133, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45119, 124)\n",
      "(169, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45229, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45319, 124)\n",
      "(232, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45492, 124)\n",
      "(253, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45686, 124)\n",
      "(186, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(45813, 124)\n",
      "(315, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46069, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46189, 124)\n",
      "(252, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46382, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46502, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46592, 124)\n",
      "(307, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46840, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(46990, 124)\n",
      "(229, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47160, 124)\n",
      "(299, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47400, 124)\n",
      "(190, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47531, 124)\n",
      "(87, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47559, 124)\n",
      "(185, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47685, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47835, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(47955, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(48015, 124)\n",
      "(237, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(48193, 124)\n",
      "(477, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(48611, 124)\n",
      "(189, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(48741, 124)\n",
      "(250, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(48932, 124)\n",
      "(282, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49155, 124)\n",
      "(198, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49294, 124)\n",
      "(290, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49525, 124)\n",
      "(123, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49589, 124)\n",
      "(143, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49673, 124)\n",
      "(146, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49760, 124)\n",
      "(186, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49887, 124)\n",
      "(158, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(49986, 124)\n",
      "(118, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(50045, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(50225, 124)\n",
      "(281, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(50447, 124)\n",
      "(43, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(50447, 124)\n",
      "(358, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(50746, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(50926, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(51076, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(51256, 124)\n",
      "(164, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(51361, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(51451, 124)\n",
      "(203, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(51595, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(51745, 124)\n",
      "(332, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52018, 124)\n",
      "(335, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52294, 124)\n",
      "(113, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52348, 124)\n",
      "(77, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52366, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52456, 124)\n",
      "(272, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52669, 124)\n",
      "(218, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52828, 124)\n",
      "(180, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(52949, 124)\n",
      "(154, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53044, 124)\n",
      "(144, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53129, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53219, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53399, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53489, 124)\n",
      "(119, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53549, 124)\n",
      "(146, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53636, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53816, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53906, 124)\n",
      "(118, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(53965, 124)\n",
      "(150, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54056, 124)\n",
      "(234, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54231, 124)\n",
      "(158, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54330, 124)\n",
      "(57, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54330, 124)\n",
      "(145, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54416, 124)\n",
      "(188, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54545, 124)\n",
      "(122, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54608, 124)\n",
      "(206, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54755, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(54845, 124)\n",
      "(346, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(55132, 124)\n",
      "(239, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(55312, 124)\n",
      "(179, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(55432, 124)\n",
      "(138, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(55511, 124)\n",
      "(304, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(55756, 124)\n",
      "(347, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56044, 124)\n",
      "(152, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56137, 124)\n",
      "(208, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56286, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56436, 124)\n",
      "(270, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56647, 124)\n",
      "(299, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56887, 124)\n",
      "(137, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(56965, 124)\n",
      "(204, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57110, 124)\n",
      "(183, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57234, 124)\n",
      "(172, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57347, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57437, 124)\n",
      "(152, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57530, 124)\n",
      "(216, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57687, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(57777, 124)\n",
      "(292, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58010, 124)\n",
      "(289, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58240, 124)\n",
      "(193, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58374, 124)\n",
      "(226, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58541, 124)\n",
      "(271, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58753, 124)\n",
      "(209, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58903, 124)\n",
      "(130, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(58974, 124)\n",
      "(208, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(59123, 124)\n",
      "(105, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(59169, 124)\n",
      "(128, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(59238, 124)\n",
      "(149, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(59328, 124)\n",
      "(125, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(59394, 124)\n",
      "(260, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(59595, 124)\n",
      "(59594, 124)\n",
      "(59594, 124)\n",
      "47675\n",
      "(47675, 120) 47675 (47675, 4)\n",
      "(47675, 30, 4) (47675, 4) (11919, 30, 4) (11919, 4)\n"
     ]
    }
   ],
   "source": [
    "# specify the number of lag hours\n",
    "n_seq = 30\n",
    "n_seq_future = 30\n",
    "n_features = 4\n",
    "np.array(reframed, dtype='float32')\n",
    "# load dataset\n",
    "#train_csv = [\"train/0_184_1327b.csv\", \"train/0_184_1329b.csv\"]\n",
    "for f in train_csv:\n",
    "    dataset = read_csv(f, header=0)\n",
    "    values = dataset.values\n",
    "    print(values.shape)\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    #print(scaler.data_max_)\n",
    "    #print(scaler.data_min_)\n",
    "\n",
    "    # frame as supervised learning\n",
    "    reframed = np.append(reframed, series_to_supervised(scaled, n_seq, n_seq_future), axis = 0)\n",
    "    print(reframed.shape)\n",
    "reframed = np.delete(reframed, 0, axis = 0)\n",
    "print(reframed.shape)\n",
    "# split into train and test sets\n",
    "values = reframed\n",
    "print(values.shape)\n",
    "n_train_num = int(reframed.shape[0] * 0.8)\n",
    "print(n_train_num)\n",
    "train = values[:n_train_num, :]\n",
    "val = values[n_train_num:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_seq * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, n_obs:n_obs+n_features]\n",
    "val_X, val_y = val[:, :n_obs], val[:, n_obs:n_obs+n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_seq, n_features))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_seq, n_features))\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 7.11267948e-01],\n",
       "       [0.00000000e+00, 9.93630648e-01, 4.82573733e-03, 7.25352287e-01],\n",
       "       [0.00000000e+00, 9.80891705e-01, 9.11528245e-03, 7.25352287e-01],\n",
       "       [5.29661018e-04, 9.74522352e-01, 1.34048238e-02, 7.32394695e-01],\n",
       "       [5.29661018e-04, 9.61783648e-01, 1.66219845e-02, 7.32394695e-01],\n",
       "       [5.29661018e-04, 9.55414057e-01, 1.98391415e-02, 7.46479034e-01],\n",
       "       [1.05932204e-03, 9.42675352e-01, 2.25201063e-02, 7.46479034e-01],\n",
       "       [1.05932204e-03, 9.36305761e-01, 2.35924907e-02, 7.53521442e-01],\n",
       "       [1.05932204e-03, 9.23567057e-01, 2.41286866e-02, 7.53521442e-01],\n",
       "       [1.58898300e-03, 9.17197466e-01, 2.52010711e-02, 7.67605782e-01],\n",
       "       [1.58898300e-03, 9.04458761e-01, 2.52010711e-02, 7.67605782e-01],\n",
       "       [1.58898300e-03, 8.98089170e-01, 2.52010711e-02, 7.74648190e-01],\n",
       "       [1.58898300e-03, 8.85350466e-01, 2.52010711e-02, 7.74648190e-01],\n",
       "       [2.11864407e-03, 8.78980875e-01, 2.57372633e-02, 7.88732529e-01],\n",
       "       [2.11864407e-03, 8.66242170e-01, 2.57372633e-02, 7.88732529e-01],\n",
       "       [1.58898300e-03, 8.53503227e-01, 2.62734592e-02, 7.81690121e-01],\n",
       "       [1.05932204e-03, 8.40764523e-01, 2.62734592e-02, 7.74648190e-01],\n",
       "       [1.58898300e-03, 8.40764523e-01, 2.89544240e-02, 7.81690121e-01],\n",
       "       [1.58898300e-03, 8.40764523e-01, 3.05630006e-02, 7.88732529e-01],\n",
       "       [2.11864407e-03, 8.40764523e-01, 3.32439654e-02, 7.95774937e-01],\n",
       "       [4.23728814e-03, 8.47133875e-01, 3.91420871e-02, 7.95774937e-01],\n",
       "       [6.88559329e-03, 8.53503227e-01, 4.55764085e-02, 8.02816868e-01],\n",
       "       [9.00423713e-03, 8.59872580e-01, 5.09383380e-02, 8.02816868e-01],\n",
       "       [1.11228814e-02, 8.66242170e-01, 5.68364635e-02, 8.02816868e-01],\n",
       "       [1.48305083e-02, 8.78980875e-01, 6.16621971e-02, 8.02816868e-01],\n",
       "       [1.80084743e-02, 8.85350466e-01, 6.54155463e-02, 8.02816868e-01],\n",
       "       [2.17161011e-02, 8.98089170e-01, 7.02412874e-02, 8.02816868e-01],\n",
       "       [2.27754246e-02, 8.98089170e-01, 7.34584406e-02, 7.95774937e-01],\n",
       "       [2.33050846e-02, 9.04458761e-01, 7.56032169e-02, 7.95774937e-01],\n",
       "       [2.43644062e-02, 9.04458761e-01, 7.88203701e-02, 7.88732529e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12182204, 0.77707005, 0.16032171, 0.78873253])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59594"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59594, 124)\n"
     ]
    }
   ],
   "source": [
    "print(reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47675, 30, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11919, 30, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the weights.\n",
    "model_checkpoint = ModelCheckpoint(filepath='30f_future_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename='bb_lstm_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                               min_delta=0.0,\n",
    "#                               patience=20,\n",
    "#                               verbose=1)\n",
    "\n",
    "#reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                         factor=0.2,\n",
    "#                                         patience=8,\n",
    "#                                         verbose=1,\n",
    "#                                         epsilon=0.001,\n",
    "#                                         cooldown=0,\n",
    "#                                         min_lr=0.00001)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger\n",
    "#             early_stopping\n",
    "             #reduce_learning_rate]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47675 samples, validate on 11919 samples\n",
      "Epoch 1/300\n",
      " - 160s - loss: 0.1649 - val_loss: 0.1459\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14588, saving model to 30f_future_epoch-01_loss-0.1649_val_loss-0.1459.h5\n",
      "Epoch 2/300\n",
      " - 158s - loss: 0.1479 - val_loss: 0.1412\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14588 to 0.14120, saving model to 30f_future_epoch-02_loss-0.1479_val_loss-0.1412.h5\n",
      "Epoch 3/300\n",
      " - 157s - loss: 0.1395 - val_loss: 0.1272\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14120 to 0.12715, saving model to 30f_future_epoch-03_loss-0.1395_val_loss-0.1272.h5\n",
      "Epoch 4/300\n",
      " - 157s - loss: 0.1240 - val_loss: 0.1219\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12715 to 0.12190, saving model to 30f_future_epoch-04_loss-0.1240_val_loss-0.1219.h5\n",
      "Epoch 5/300\n",
      " - 157s - loss: 0.1215 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12190 to 0.11888, saving model to 30f_future_epoch-05_loss-0.1215_val_loss-0.1189.h5\n",
      "Epoch 6/300\n",
      " - 158s - loss: 0.1233 - val_loss: 0.1202\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11888\n",
      "Epoch 7/300\n",
      " - 158s - loss: 0.1198 - val_loss: 0.1304\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11888\n",
      "Epoch 8/300\n",
      " - 158s - loss: 0.1252 - val_loss: 0.1220\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11888\n",
      "Epoch 9/300\n",
      " - 157s - loss: 0.1209 - val_loss: 0.1264\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11888\n",
      "Epoch 10/300\n",
      " - 157s - loss: 0.1238 - val_loss: 0.1166\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11888 to 0.11665, saving model to 30f_future_epoch-10_loss-0.1238_val_loss-0.1166.h5\n",
      "Epoch 11/300\n",
      " - 156s - loss: 0.1132 - val_loss: 0.1082\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11665 to 0.10817, saving model to 30f_future_epoch-11_loss-0.1132_val_loss-0.1082.h5\n",
      "Epoch 12/300\n",
      " - 156s - loss: 0.1080 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10817 to 0.10201, saving model to 30f_future_epoch-12_loss-0.1080_val_loss-0.1020.h5\n",
      "Epoch 13/300\n",
      " - 158s - loss: 0.1039 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.10201 to 0.10007, saving model to 30f_future_epoch-13_loss-0.1039_val_loss-0.1001.h5\n",
      "Epoch 14/300\n",
      " - 159s - loss: 0.1011 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.10007 to 0.09608, saving model to 30f_future_epoch-14_loss-0.1011_val_loss-0.0961.h5\n",
      "Epoch 15/300\n",
      " - 158s - loss: 0.0988 - val_loss: 0.0951\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09608 to 0.09511, saving model to 30f_future_epoch-15_loss-0.0988_val_loss-0.0951.h5\n",
      "Epoch 16/300\n",
      " - 157s - loss: 0.0984 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09511 to 0.09458, saving model to 30f_future_epoch-16_loss-0.0984_val_loss-0.0946.h5\n",
      "Epoch 17/300\n",
      " - 158s - loss: 0.0963 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09458\n",
      "Epoch 18/300\n",
      " - 158s - loss: 0.0936 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09458 to 0.09063, saving model to 30f_future_epoch-18_loss-0.0936_val_loss-0.0906.h5\n",
      "Epoch 19/300\n",
      " - 161s - loss: 0.0920 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09063 to 0.08848, saving model to 30f_future_epoch-19_loss-0.0920_val_loss-0.0885.h5\n",
      "Epoch 20/300\n",
      " - 160s - loss: 0.0911 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08848\n",
      "Epoch 21/300\n",
      " - 159s - loss: 0.0904 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.08848\n",
      "Epoch 22/300\n",
      " - 159s - loss: 0.0896 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08848 to 0.08781, saving model to 30f_future_epoch-22_loss-0.0896_val_loss-0.0878.h5\n",
      "Epoch 23/300\n",
      " - 158s - loss: 0.0887 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08781 to 0.08534, saving model to 30f_future_epoch-23_loss-0.0887_val_loss-0.0853.h5\n",
      "Epoch 24/300\n",
      " - 159s - loss: 0.0887 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08534\n",
      "Epoch 25/300\n",
      " - 159s - loss: 0.0877 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08534\n",
      "Epoch 26/300\n",
      " - 160s - loss: 0.0873 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08534\n",
      "Epoch 27/300\n",
      " - 158s - loss: 0.0869 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.08534\n",
      "Epoch 28/300\n",
      " - 160s - loss: 0.0865 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.08534\n",
      "Epoch 29/300\n",
      " - 159s - loss: 0.0861 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08534 to 0.08266, saving model to 30f_future_epoch-29_loss-0.0861_val_loss-0.0827.h5\n",
      "Epoch 30/300\n",
      " - 160s - loss: 0.0859 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.08266\n",
      "Epoch 31/300\n",
      " - 160s - loss: 0.0856 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.08266\n",
      "Epoch 32/300\n",
      " - 160s - loss: 0.0853 - val_loss: 0.0877\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.08266\n",
      "Epoch 33/300\n",
      " - 159s - loss: 0.0848 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.08266 to 0.08174, saving model to 30f_future_epoch-33_loss-0.0848_val_loss-0.0817.h5\n",
      "Epoch 34/300\n",
      " - 160s - loss: 0.0846 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.08174\n",
      "Epoch 35/300\n",
      " - 159s - loss: 0.0843 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.08174 to 0.08121, saving model to 30f_future_epoch-35_loss-0.0843_val_loss-0.0812.h5\n",
      "Epoch 36/300\n",
      " - 161s - loss: 0.0840 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.08121\n",
      "Epoch 37/300\n",
      " - 160s - loss: 0.0838 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.08121\n",
      "Epoch 38/300\n",
      " - 159s - loss: 0.0839 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.08121\n",
      "Epoch 39/300\n",
      " - 158s - loss: 0.0835 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.08121\n",
      "Epoch 40/300\n",
      " - 158s - loss: 0.0834 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.08121\n",
      "Epoch 41/300\n",
      " - 160s - loss: 0.0833 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.08121\n",
      "Epoch 42/300\n",
      " - 160s - loss: 0.0831 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.08121\n",
      "Epoch 43/300\n",
      " - 160s - loss: 0.0828 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.08121\n",
      "Epoch 44/300\n",
      " - 161s - loss: 0.0826 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.08121\n",
      "Epoch 45/300\n",
      " - 160s - loss: 0.0824 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08121\n",
      "Epoch 46/300\n",
      " - 158s - loss: 0.0820 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08121\n",
      "Epoch 47/300\n",
      " - 158s - loss: 0.0821 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08121\n",
      "Epoch 48/300\n",
      " - 157s - loss: 0.0818 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.08121\n",
      "Epoch 49/300\n",
      " - 156s - loss: 0.0815 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.08121\n",
      "Epoch 50/300\n",
      " - 157s - loss: 0.0815 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.08121 to 0.08102, saving model to 30f_future_epoch-50_loss-0.0815_val_loss-0.0810.h5\n",
      "Epoch 51/300\n",
      " - 158s - loss: 0.0812 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.08102\n",
      "Epoch 52/300\n",
      " - 158s - loss: 0.0808 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08102\n",
      "Epoch 53/300\n",
      " - 159s - loss: 0.0806 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.08102\n",
      "Epoch 54/300\n",
      " - 159s - loss: 0.0804 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.08102 to 0.08045, saving model to 30f_future_epoch-54_loss-0.0804_val_loss-0.0804.h5\n",
      "Epoch 55/300\n",
      " - 160s - loss: 0.0805 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.08045\n",
      "Epoch 56/300\n",
      " - 160s - loss: 0.0802 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.08045\n",
      "Epoch 57/300\n",
      " - 158s - loss: 0.0800 - val_loss: 0.0800\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.08045 to 0.07997, saving model to 30f_future_epoch-57_loss-0.0800_val_loss-0.0800.h5\n",
      "Epoch 58/300\n",
      " - 157s - loss: 0.0800 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07997\n",
      "Epoch 59/300\n",
      " - 157s - loss: 0.0797 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07997\n",
      "Epoch 60/300\n",
      " - 158s - loss: 0.0794 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07997\n",
      "Epoch 61/300\n",
      " - 158s - loss: 0.0793 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.07997 to 0.07983, saving model to 30f_future_epoch-61_loss-0.0793_val_loss-0.0798.h5\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 158s - loss: 0.0789 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07983\n",
      "Epoch 63/300\n",
      " - 158s - loss: 0.0788 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.07983 to 0.07826, saving model to 30f_future_epoch-63_loss-0.0788_val_loss-0.0783.h5\n",
      "Epoch 64/300\n",
      " - 159s - loss: 0.0792 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.07826 to 0.07825, saving model to 30f_future_epoch-64_loss-0.0792_val_loss-0.0783.h5\n",
      "Epoch 65/300\n",
      " - 159s - loss: 0.0787 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07825\n",
      "Epoch 66/300\n",
      " - 158s - loss: 0.0786 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07825\n",
      "Epoch 67/300\n",
      " - 159s - loss: 0.0792 - val_loss: 0.0771\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.07825 to 0.07708, saving model to 30f_future_epoch-67_loss-0.0792_val_loss-0.0771.h5\n",
      "Epoch 68/300\n",
      " - 158s - loss: 0.0780 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07708\n",
      "Epoch 69/300\n",
      " - 160s - loss: 0.0783 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07708\n",
      "Epoch 70/300\n",
      " - 160s - loss: 0.0782 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07708\n",
      "Epoch 71/300\n",
      " - 161s - loss: 0.0783 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07708\n",
      "Epoch 72/300\n",
      " - 159s - loss: 0.0781 - val_loss: 0.0799\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07708\n",
      "Epoch 73/300\n",
      " - 157s - loss: 0.0780 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07708\n",
      "Epoch 74/300\n",
      " - 158s - loss: 0.0779 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07708\n",
      "Epoch 75/300\n",
      " - 157s - loss: 0.0778 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07708\n",
      "Epoch 76/300\n",
      " - 158s - loss: 0.0777 - val_loss: 0.0800\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07708\n",
      "Epoch 77/300\n",
      " - 156s - loss: 0.0777 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07708\n",
      "Epoch 78/300\n",
      " - 155s - loss: 0.0773 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07708\n",
      "Epoch 79/300\n",
      " - 156s - loss: 0.0775 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07708\n",
      "Epoch 80/300\n",
      " - 156s - loss: 0.0774 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07708\n",
      "Epoch 81/300\n",
      " - 157s - loss: 0.0772 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07708\n",
      "Epoch 82/300\n",
      " - 157s - loss: 0.0771 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07708\n",
      "Epoch 83/300\n",
      " - 158s - loss: 0.0769 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07708\n",
      "Epoch 84/300\n",
      " - 157s - loss: 0.0768 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07708\n",
      "Epoch 85/300\n",
      " - 155s - loss: 0.0764 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07708\n",
      "Epoch 86/300\n",
      " - 156s - loss: 0.0765 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07708\n",
      "Epoch 87/300\n",
      " - 158s - loss: 0.0766 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.07708\n",
      "Epoch 88/300\n",
      " - 158s - loss: 0.0764 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.07708\n",
      "Epoch 89/300\n",
      " - 157s - loss: 0.0761 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.07708\n",
      "Epoch 90/300\n",
      " - 158s - loss: 0.0763 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.07708\n",
      "Epoch 91/300\n",
      " - 156s - loss: 0.0761 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.07708\n",
      "Epoch 92/300\n",
      " - 156s - loss: 0.0757 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.07708\n",
      "Epoch 93/300\n",
      " - 156s - loss: 0.0758 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.07708\n",
      "Epoch 94/300\n",
      " - 157s - loss: 0.0758 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.07708\n",
      "Epoch 95/300\n",
      " - 158s - loss: 0.0756 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.07708\n",
      "Epoch 96/300\n",
      " - 158s - loss: 0.0755 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.07708\n",
      "Epoch 97/300\n",
      " - 159s - loss: 0.0754 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.07708\n",
      "Epoch 98/300\n",
      " - 159s - loss: 0.0754 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07708\n",
      "Epoch 99/300\n",
      " - 158s - loss: 0.0751 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07708\n",
      "Epoch 100/300\n",
      " - 158s - loss: 0.0750 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07708\n",
      "Epoch 101/300\n",
      " - 159s - loss: 0.0751 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.07708\n",
      "Epoch 102/300\n",
      " - 158s - loss: 0.0748 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.07708\n",
      "Epoch 103/300\n",
      " - 158s - loss: 0.0748 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.07708\n",
      "Epoch 104/300\n",
      " - 159s - loss: 0.0746 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.07708\n",
      "Epoch 105/300\n",
      " - 160s - loss: 0.0747 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.07708\n",
      "Epoch 106/300\n",
      " - 159s - loss: 0.0745 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.07708\n",
      "Epoch 107/300\n",
      " - 159s - loss: 0.0747 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.07708\n",
      "Epoch 108/300\n",
      " - 160s - loss: 0.0747 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.07708\n",
      "Epoch 109/300\n",
      " - 160s - loss: 0.0743 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.07708\n",
      "Epoch 110/300\n",
      " - 161s - loss: 0.0746 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.07708\n",
      "Epoch 111/300\n",
      " - 159s - loss: 0.0740 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.07708\n",
      "Epoch 112/300\n",
      " - 160s - loss: 0.0740 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.07708\n",
      "Epoch 113/300\n",
      " - 159s - loss: 0.0739 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.07708\n",
      "Epoch 114/300\n",
      " - 157s - loss: 0.0739 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.07708\n",
      "Epoch 115/300\n",
      " - 158s - loss: 0.0740 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.07708\n",
      "Epoch 116/300\n",
      " - 158s - loss: 0.0739 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.07708\n",
      "Epoch 117/300\n",
      " - 158s - loss: 0.0741 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.07708\n",
      "Epoch 118/300\n",
      " - 160s - loss: 0.0737 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.07708\n",
      "Epoch 119/300\n",
      " - 159s - loss: 0.0738 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.07708\n",
      "Epoch 120/300\n",
      " - 160s - loss: 0.0737 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.07708\n",
      "Epoch 121/300\n",
      " - 159s - loss: 0.0736 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.07708\n",
      "Epoch 122/300\n",
      " - 158s - loss: 0.0736 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.07708\n",
      "Epoch 123/300\n",
      " - 159s - loss: 0.0734 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.07708\n",
      "Epoch 124/300\n",
      " - 157s - loss: 0.0733 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.07708\n",
      "Epoch 125/300\n",
      " - 156s - loss: 0.0734 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.07708\n",
      "Epoch 126/300\n",
      " - 158s - loss: 0.0734 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.07708\n",
      "Epoch 127/300\n",
      " - 160s - loss: 0.0734 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.07708\n",
      "Epoch 128/300\n",
      " - 159s - loss: 0.0735 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.07708\n",
      "Epoch 129/300\n",
      " - 159s - loss: 0.0732 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.07708\n",
      "Epoch 130/300\n",
      " - 157s - loss: 0.0728 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.07708\n",
      "Epoch 131/300\n",
      " - 158s - loss: 0.0733 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.07708\n",
      "Epoch 132/300\n",
      " - 158s - loss: 0.0734 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07708\n",
      "Epoch 133/300\n",
      " - 158s - loss: 0.0727 - val_loss: 0.0800\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.07708\n",
      "Epoch 134/300\n",
      " - 159s - loss: 0.0729 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.07708 to 0.07681, saving model to 30f_future_epoch-134_loss-0.0729_val_loss-0.0768.h5\n",
      "Epoch 135/300\n",
      " - 160s - loss: 0.0727 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.07681\n",
      "Epoch 136/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 158s - loss: 0.0728 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.07681\n",
      "Epoch 137/300\n",
      " - 160s - loss: 0.0727 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.07681\n",
      "Epoch 138/300\n",
      " - 159s - loss: 0.0724 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.07681\n",
      "Epoch 139/300\n",
      " - 160s - loss: 0.0728 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.07681\n",
      "Epoch 140/300\n",
      " - 159s - loss: 0.0722 - val_loss: 0.0770\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.07681\n",
      "Epoch 141/300\n",
      " - 159s - loss: 0.0724 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.07681\n",
      "Epoch 142/300\n",
      " - 160s - loss: 0.0720 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.07681\n",
      "Epoch 143/300\n",
      " - 159s - loss: 0.0723 - val_loss: 0.0769\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.07681\n",
      "Epoch 144/300\n",
      " - 159s - loss: 0.0724 - val_loss: 0.0771\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.07681\n",
      "Epoch 145/300\n",
      " - 159s - loss: 0.0722 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.07681\n",
      "Epoch 146/300\n",
      " - 161s - loss: 0.0722 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.07681 to 0.07593, saving model to 30f_future_epoch-146_loss-0.0722_val_loss-0.0759.h5\n",
      "Epoch 147/300\n",
      " - 160s - loss: 0.0724 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.07593 to 0.07589, saving model to 30f_future_epoch-147_loss-0.0724_val_loss-0.0759.h5\n",
      "Epoch 148/300\n",
      " - 160s - loss: 0.0719 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.07589\n",
      "Epoch 149/300\n",
      " - 160s - loss: 0.0719 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.07589\n",
      "Epoch 150/300\n",
      " - 159s - loss: 0.0718 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.07589\n",
      "Epoch 151/300\n",
      " - 160s - loss: 0.0719 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.07589\n",
      "Epoch 152/300\n",
      " - 160s - loss: 0.0718 - val_loss: 0.0766\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.07589\n",
      "Epoch 153/300\n",
      " - 158s - loss: 0.0716 - val_loss: 0.0763\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.07589\n",
      "Epoch 154/300\n",
      " - 159s - loss: 0.0719 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.07589 to 0.07567, saving model to 30f_future_epoch-154_loss-0.0719_val_loss-0.0757.h5\n",
      "Epoch 155/300\n",
      " - 159s - loss: 0.0716 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.07567\n",
      "Epoch 156/300\n",
      " - 157s - loss: 0.0714 - val_loss: 0.0766\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.07567\n",
      "Epoch 157/300\n",
      " - 156s - loss: 0.0714 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.07567\n",
      "Epoch 158/300\n",
      " - 158s - loss: 0.0710 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.07567\n",
      "Epoch 159/300\n",
      " - 158s - loss: 0.0714 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.07567\n",
      "Epoch 160/300\n",
      " - 158s - loss: 0.0710 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.07567 to 0.07551, saving model to 30f_future_epoch-160_loss-0.0710_val_loss-0.0755.h5\n",
      "Epoch 161/300\n",
      " - 159s - loss: 0.0708 - val_loss: 0.0748\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.07551 to 0.07476, saving model to 30f_future_epoch-161_loss-0.0708_val_loss-0.0748.h5\n",
      "Epoch 162/300\n",
      " - 159s - loss: 0.0710 - val_loss: 0.0770\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.07476\n",
      "Epoch 163/300\n",
      " - 158s - loss: 0.0709 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.07476\n",
      "Epoch 164/300\n",
      " - 159s - loss: 0.0709 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.07476\n",
      "Epoch 165/300\n",
      " - 160s - loss: 0.0710 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.07476\n",
      "Epoch 166/300\n",
      " - 160s - loss: 0.0707 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.07476\n",
      "Epoch 167/300\n",
      " - 160s - loss: 0.0703 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.07476\n",
      "Epoch 168/300\n",
      " - 160s - loss: 0.0703 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.07476\n",
      "Epoch 169/300\n",
      " - 160s - loss: 0.0703 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.07476\n",
      "Epoch 170/300\n",
      " - 159s - loss: 0.0702 - val_loss: 0.0766\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.07476\n",
      "Epoch 171/300\n",
      " - 159s - loss: 0.0701 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.07476\n",
      "Epoch 172/300\n",
      " - 160s - loss: 0.0699 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.07476\n",
      "Epoch 173/300\n",
      " - 160s - loss: 0.0701 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.07476\n",
      "Epoch 174/300\n",
      " - 157s - loss: 0.0703 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.07476\n",
      "Epoch 175/300\n",
      " - 157s - loss: 0.0702 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.07476\n",
      "Epoch 176/300\n",
      " - 157s - loss: 0.0702 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.07476\n",
      "Epoch 177/300\n",
      " - 158s - loss: 0.0698 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.07476\n",
      "Epoch 178/300\n",
      " - 158s - loss: 0.0703 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.07476\n",
      "Epoch 179/300\n",
      " - 155s - loss: 0.0700 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.07476\n",
      "Epoch 180/300\n",
      " - 155s - loss: 0.0698 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.07476\n",
      "Epoch 181/300\n",
      " - 154s - loss: 0.0698 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.07476\n",
      "Epoch 182/300\n",
      " - 156s - loss: 0.0696 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.07476\n",
      "Epoch 183/300\n",
      " - 156s - loss: 0.0695 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.07476\n",
      "Epoch 184/300\n",
      " - 156s - loss: 0.0699 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.07476\n",
      "Epoch 185/300\n",
      " - 159s - loss: 0.0696 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.07476\n",
      "Epoch 186/300\n",
      " - 160s - loss: 0.0692 - val_loss: 0.0843\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.07476\n",
      "Epoch 187/300\n",
      " - 160s - loss: 0.0692 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.07476\n",
      "Epoch 188/300\n",
      " - 160s - loss: 0.0693 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.07476\n",
      "Epoch 189/300\n",
      " - 160s - loss: 0.0695 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.07476\n",
      "Epoch 190/300\n",
      " - 159s - loss: 0.0688 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.07476\n",
      "Epoch 191/300\n",
      " - 159s - loss: 0.0695 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.07476\n",
      "Epoch 192/300\n",
      " - 159s - loss: 0.0689 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.07476\n",
      "Epoch 193/300\n",
      " - 159s - loss: 0.0691 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.07476\n",
      "Epoch 194/300\n",
      " - 159s - loss: 0.0695 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.07476\n",
      "Epoch 195/300\n",
      " - 160s - loss: 0.0688 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.07476\n",
      "Epoch 196/300\n",
      " - 159s - loss: 0.0692 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.07476\n",
      "Epoch 197/300\n",
      " - 159s - loss: 0.0686 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.07476\n",
      "Epoch 198/300\n",
      " - 155s - loss: 0.0689 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.07476\n",
      "Epoch 199/300\n",
      " - 157s - loss: 0.0689 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.07476\n",
      "Epoch 200/300\n",
      " - 158s - loss: 0.0687 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.07476\n",
      "Epoch 201/300\n",
      " - 158s - loss: 0.0687 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.07476\n",
      "Epoch 202/300\n",
      " - 160s - loss: 0.0686 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.07476\n",
      "Epoch 203/300\n",
      " - 160s - loss: 0.0685 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.07476\n",
      "Epoch 204/300\n",
      " - 160s - loss: 0.0687 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.07476\n",
      "Epoch 205/300\n",
      " - 158s - loss: 0.0687 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.07476\n",
      "Epoch 206/300\n",
      " - 157s - loss: 0.0684 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.07476\n",
      "Epoch 207/300\n",
      " - 159s - loss: 0.0684 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.07476\n",
      "Epoch 208/300\n",
      " - 158s - loss: 0.0685 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.07476\n",
      "Epoch 209/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 158s - loss: 0.0690 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.07476\n",
      "Epoch 210/300\n",
      " - 159s - loss: 0.0684 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.07476\n",
      "Epoch 211/300\n",
      " - 159s - loss: 0.0688 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.07476\n",
      "Epoch 212/300\n",
      " - 159s - loss: 0.0682 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.07476\n",
      "Epoch 213/300\n",
      " - 158s - loss: 0.0681 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.07476\n",
      "Epoch 214/300\n",
      " - 158s - loss: 0.0680 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.07476\n",
      "Epoch 215/300\n",
      " - 158s - loss: 0.0681 - val_loss: 0.0787\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.07476\n",
      "Epoch 216/300\n",
      " - 159s - loss: 0.0677 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.07476\n",
      "Epoch 217/300\n",
      " - 158s - loss: 0.0678 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.07476\n",
      "Epoch 218/300\n",
      " - 158s - loss: 0.0678 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.07476\n",
      "Epoch 219/300\n",
      " - 158s - loss: 0.0678 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.07476\n",
      "Epoch 220/300\n",
      " - 157s - loss: 0.0678 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.07476\n",
      "Epoch 221/300\n",
      " - 156s - loss: 0.0676 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.07476\n",
      "Epoch 222/300\n",
      " - 157s - loss: 0.0684 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.07476\n",
      "Epoch 223/300\n",
      " - 159s - loss: 0.0680 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.07476\n",
      "Epoch 224/300\n",
      " - 160s - loss: 0.0679 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.07476\n",
      "Epoch 225/300\n",
      " - 159s - loss: 0.0684 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.07476\n",
      "Epoch 226/300\n",
      " - 159s - loss: 0.0680 - val_loss: 0.0787\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.07476\n",
      "Epoch 227/300\n",
      " - 158s - loss: 0.0675 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.07476\n",
      "Epoch 228/300\n",
      " - 158s - loss: 0.0670 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.07476\n",
      "Epoch 229/300\n",
      " - 157s - loss: 0.0673 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.07476\n",
      "Epoch 230/300\n",
      " - 159s - loss: 0.0672 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.07476\n",
      "Epoch 231/300\n",
      " - 157s - loss: 0.0674 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.07476\n",
      "Epoch 232/300\n",
      " - 157s - loss: 0.0674 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.07476\n",
      "Epoch 233/300\n",
      " - 157s - loss: 0.0670 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.07476\n",
      "Epoch 234/300\n",
      " - 159s - loss: 0.0671 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.07476\n",
      "Epoch 235/300\n",
      " - 158s - loss: 0.0672 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.07476\n",
      "Epoch 236/300\n",
      " - 156s - loss: 0.0670 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.07476\n",
      "Epoch 237/300\n",
      " - 158s - loss: 0.0669 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.07476\n",
      "Epoch 238/300\n",
      " - 158s - loss: 0.0669 - val_loss: 0.0799\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.07476\n",
      "Epoch 239/300\n",
      " - 159s - loss: 0.0669 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.07476\n",
      "Epoch 240/300\n",
      " - 159s - loss: 0.0667 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.07476\n",
      "Epoch 241/300\n",
      " - 159s - loss: 0.0668 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.07476\n",
      "Epoch 242/300\n",
      " - 159s - loss: 0.0669 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.07476\n",
      "Epoch 243/300\n",
      " - 157s - loss: 0.0671 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.07476\n",
      "Epoch 244/300\n",
      " - 157s - loss: 0.0669 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.07476\n",
      "Epoch 245/300\n",
      " - 156s - loss: 0.0669 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.07476\n",
      "Epoch 246/300\n",
      " - 155s - loss: 0.0669 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.07476\n",
      "Epoch 247/300\n",
      " - 155s - loss: 0.0668 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.07476\n",
      "Epoch 248/300\n",
      " - 157s - loss: 0.0667 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.07476\n",
      "Epoch 249/300\n",
      " - 157s - loss: 0.0669 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.07476\n",
      "Epoch 250/300\n",
      " - 157s - loss: 0.0667 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.07476\n",
      "Epoch 251/300\n",
      " - 156s - loss: 0.0667 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.07476\n",
      "Epoch 252/300\n",
      " - 155s - loss: 0.0665 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.07476\n",
      "Epoch 253/300\n",
      " - 156s - loss: 0.0665 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.07476\n",
      "Epoch 254/300\n",
      " - 158s - loss: 0.0663 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.07476\n",
      "Epoch 255/300\n",
      " - 157s - loss: 0.0663 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.07476\n",
      "Epoch 256/300\n",
      " - 157s - loss: 0.0666 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.07476\n",
      "Epoch 257/300\n",
      " - 158s - loss: 0.0661 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.07476\n",
      "Epoch 258/300\n",
      " - 156s - loss: 0.0663 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.07476\n",
      "Epoch 259/300\n",
      " - 156s - loss: 0.0663 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.07476\n",
      "Epoch 260/300\n",
      " - 156s - loss: 0.0658 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.07476\n",
      "Epoch 261/300\n",
      " - 157s - loss: 0.0659 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.07476\n",
      "Epoch 262/300\n",
      " - 159s - loss: 0.0659 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.07476\n",
      "Epoch 263/300\n",
      " - 158s - loss: 0.0663 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.07476\n",
      "Epoch 264/300\n",
      " - 157s - loss: 0.0656 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.07476\n",
      "Epoch 265/300\n",
      " - 158s - loss: 0.0666 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.07476\n",
      "Epoch 266/300\n",
      " - 157s - loss: 0.0657 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.07476\n",
      "Epoch 267/300\n",
      " - 155s - loss: 0.0658 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.07476\n",
      "Epoch 268/300\n",
      " - 156s - loss: 0.0658 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.07476\n",
      "Epoch 269/300\n",
      " - 156s - loss: 0.0655 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.07476\n",
      "Epoch 270/300\n",
      " - 156s - loss: 0.0655 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.07476\n",
      "Epoch 271/300\n",
      " - 159s - loss: 0.0655 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.07476\n",
      "Epoch 272/300\n",
      " - 158s - loss: 0.0652 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.07476\n",
      "Epoch 273/300\n",
      " - 160s - loss: 0.0653 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.07476\n",
      "Epoch 274/300\n",
      " - 159s - loss: 0.0657 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.07476\n",
      "Epoch 275/300\n",
      " - 160s - loss: 0.0657 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.07476\n",
      "Epoch 276/300\n",
      " - 159s - loss: 0.0651 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.07476\n",
      "Epoch 277/300\n",
      " - 158s - loss: 0.0656 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.07476\n",
      "Epoch 278/300\n",
      " - 158s - loss: 0.0650 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.07476\n",
      "Epoch 279/300\n",
      " - 159s - loss: 0.0649 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.07476\n",
      "Epoch 280/300\n",
      " - 159s - loss: 0.0650 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.07476\n",
      "Epoch 281/300\n",
      " - 159s - loss: 0.0647 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.07476\n",
      "Epoch 282/300\n",
      " - 159s - loss: 0.0649 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.07476\n",
      "Epoch 283/300\n",
      " - 159s - loss: 0.0647 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.07476\n",
      "Epoch 284/300\n",
      " - 158s - loss: 0.0645 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.07476\n",
      "Epoch 285/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 156s - loss: 0.0651 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.07476\n",
      "Epoch 286/300\n",
      " - 157s - loss: 0.0647 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.07476\n",
      "Epoch 287/300\n",
      " - 157s - loss: 0.0644 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.07476\n",
      "Epoch 288/300\n",
      " - 156s - loss: 0.0647 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.07476\n",
      "Epoch 289/300\n",
      " - 158s - loss: 0.0650 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.07476\n",
      "Epoch 290/300\n",
      " - 160s - loss: 0.0645 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.07476\n",
      "Epoch 291/300\n",
      " - 160s - loss: 0.0649 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.07476\n",
      "Epoch 292/300\n",
      " - 157s - loss: 0.0648 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.07476\n",
      "Epoch 293/300\n",
      " - 156s - loss: 0.0647 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.07476\n",
      "Epoch 294/300\n",
      " - 156s - loss: 0.0640 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.07476\n",
      "Epoch 295/300\n",
      " - 156s - loss: 0.0646 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.07476\n",
      "Epoch 296/300\n",
      " - 158s - loss: 0.0646 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.07476\n",
      "Epoch 297/300\n",
      " - 155s - loss: 0.0640 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.07476\n",
      "Epoch 298/300\n",
      " - 157s - loss: 0.0639 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.07476\n",
      "Epoch 299/300\n",
      " - 157s - loss: 0.0643 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.07476\n",
      "Epoch 300/300\n",
      " - 156s - loss: 0.0640 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.07476\n"
     ]
    }
   ],
   "source": [
    " # design network\n",
    "K.clear_session()  # Clear previous models from memory.    \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(4))\n",
    "#adam = optimizers.Adam(lr=0.01, decay=0.0001)\n",
    "model.compile(loss='mae', optimizer=\"adam\")\n",
    "#A loss function or objective function, or optimization score function\n",
    "#mean_absolute_error\n",
    "\n",
    "epochs = 300\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=16, \n",
    "                    validation_data=(val_X, val_y), \n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VOW5wPHfM5NJMlkJ2YCEJeyLIAIiuOCKglVxr7W26vWWblpvW63aWuvS9rb21ra2atVKtVr3uqCAgog7KIvIDgl7EkIWsi+TWd77xzshISQQIGSSmef7+eSTyTlnZp6TM/Oc9zznPe8RYwxKKaUigyPUASillOo6mvSVUiqCaNJXSqkIoklfKaUiiCZ9pZSKIJr0lVIqgmjSV0qpCKJJXymlIogmfaWUiiBRoQ6gtbS0NDNo0KBQh6GUUj3KypUrS40x6Ydbrtsl/UGDBrFixYpQh6GUUj2KiOzsyHJa3lFKqQiiSV8ppSKIJn2llIog3a6mr5RSR8Pr9ZKfn09DQ0OoQzmuYmNjyc7OxuVyHdXzNekrpcJCfn4+iYmJDBo0CBEJdTjHhTGGsrIy8vPzycnJOarX0PKOUiosNDQ0kJqaGrYJH0BESE1NPaajGU36SqmwEc4Jv8mxrmPYJP0aj4+HFm1h9e6KUIeilFLdVtgk/UZfgIcX57J6V3moQ1FKRaCKigoeffTRI37ehRdeSEVF1zVWwybpx0TZVWn0B0IciVIqErWX9H0+3yGfN3/+fHr16nW8wjpI2PTeaUr6Hq8mfaVU17vzzjvZunUr48ePx+VyERsbS0pKCps2bWLLli1ceuml7N69m4aGBm699VZmz54NNA89U1NTw8yZMzn99NP57LPPyMrK4s0338TtdndqnGGT9KOcDpwOwePTpK9UpLvvrfVsKKzq1Ncc3S+JX108pt35v/vd71i3bh2rV6/mgw8+4Gtf+xrr1q3b37Vyzpw59O7dm/r6ek4++WSuuOIKUlNTD3iN3NxcXnjhBZ588kmuvvpq/vOf/3Ddddd16nqETdIHiHY68Pj8oQ5DKaWYPHnyAX3pH374YV5//XUAdu/eTW5u7kFJPycnh/HjxwMwceJEduzY0elxhVXSj3E5tKWvlDpki7yrxMfH73/8wQcf8N5777F06VLi4uI466yz2uxrHxMTs/+x0+mkvr6+0+Pq0IlcEZkhIptFJE9E7mxj/jQRWSUiPhG5stW8ASKyUEQ2isgGERnUOaEfLCbKQaMmfaVUCCQmJlJdXd3mvMrKSlJSUoiLi2PTpk0sW7asi6NrdtiWvog4gUeA6UA+sFxE5hpjNrRYbBdwA3BbGy/xL+A3xphFIpIAHLesHBPl1Ja+UiokUlNTOe200zjhhBNwu91kZmbunzdjxgz+/ve/M2rUKEaMGMGUKVNCFmdHyjuTgTxjzDYAEXkRmAXsT/rGmB3BeQdkXBEZDUQZYxYFl6vpnLDbFhOlNX2lVOg8//zzbU6PiYlhwYIFbc5rqtunpaWxbt26/dNvu62tNvSx60h5JwvY3eLv/OC0jhgOVIjIayLypYj8IXjkcFxERzm0y6ZSSh3C8b44Kwo4A1v2ORkYjC0DHUBEZovIChFZUVJSctRvZlv6mvSVUqo9HUn6BUD/Fn9nB6d1RD6w2hizzRjjA94AJrReyBjzhDFmkjFmUnr6Ye/r266YKKeeyFVKqUPoSNJfDgwTkRwRiQauAeZ28PWXA71EpCmTn0OLcwGdzXbZ1Jq+Ukq157BJP9hCvxl4F9gIvGyMWS8i94vIJQAicrKI5ANXAY+LyPrgc/3Y0s5iEVkLCPDk8VkVLe8opdThdOjiLGPMfGB+q2n3tHi8HFv2aeu5i4BxxxBjh2mXTaWUOrSwGWUTmnrvaHlHKdX1jnZoZYA///nP1NXVdXJEbQurpB8T5dChlZVSIdFTkn54jb0T5dR++kqpkGg5tPL06dPJyMjg5ZdfxuPxcNlll3HfffdRW1vL1VdfTX5+Pn6/n1/+8pfs3buXwsJCzj77bNLS0liyZMlxjTO8kr4OuKaUAlhwJxSt7dzX7DMWZv6u3dkth1ZeuHAhr776Kl988QXGGC655BI++ugjSkpK6NevH/PmzQPsmDzJyck89NBDLFmyhLS0tM6NuQ1hWd4JBEyoQ1FKRbCFCxeycOFCTjrpJCZMmMCmTZvIzc1l7NixLFq0iDvuuIOPP/6Y5OTkLo8tvFr6UXaEh0Z/gFjHcRvtQSnV3R2iRd4VjDHcddddfPe73z1o3qpVq5g/fz5333035557Lvfcc08br3D8hFVLP7rplola4lFKdbGWQytfcMEFzJkzh5oaO8ZkQUEBxcXFFBYWEhcXx3XXXcftt9/OqlWrDnru8RZmLf2mpO8HXKENRikVUVoOrTxz5kyuvfZapk6dCkBCQgLPPfcceXl53H777TgcDlwuF4899hgAs2fPZsaMGfTr109P5B4JvTm6UiqUWg+tfOuttx7w95AhQ7jgggsOet4tt9zCLbfcclxjaxJW5Z0Yl63ja3lHKaXaFl5J/4DyjlJKqdbCKuk3ncjV4ZWVikzGhH937WNdx7BK+jHae0epiBUbG0tZWVlYJ35jDGVlZcTGxh71a4TZiVyt6SsVqbKzs8nPz+dY7r7XE8TGxpKd3eagxh0SZkm/qfeO1vSVijQul4ucnJxQh9HthVV5J9al5R2llDqUsEr6+4dh0KSvlFJtCqukr8MwKKXUoYVV0m+q6TdoTV8ppdoUVkk/Icael67x+EIciVJKdU9hlfSjnA7io51U1XtDHYpSSnVL4ZP0vQ2w/SOGxVZQ1aBJXyml2hI+Sd9TBc9czHTHSqrqtbyjlFJtCZ+kH5cGzmiynPu0pa+UUu0In6TvcEBiXzLRpK+UUu0Jn6QPkJxNRqBUyztKKdWO8Er6Sf1I8ZdqS18ppdoRZkk/iyRvCdX1nrAeXlUppY5W2CX9KOMlxVRT26hX5SqlVGvhlfSTswDoI2V6gZZSSrWhQ0lfRGaIyGYRyRORO9uYP01EVomIT0SubGN+kojki8jfOiPodiX1A6CflGldXyml2nDYpC8iTuARYCYwGviGiIxutdgu4Abg+XZe5gHgo6MPs4MSbdLPlHLtwaOUUm3oSEt/MpBnjNlmjGkEXgRmtVzAGLPDGLMGOGhMYxGZCGQCCzsh3kOLTQIgngYt7yilVBs6kvSzgN0t/s4PTjssEXEAfwRuO/LQjkKUvVmwWxq1vKOUUm043idyfwDMN8bkH2ohEZktIitEZMUx3dRYBBPlJoZGKrWlr5RSB+nIjdELgP4t/s4OTuuIqcAZIvIDIAGIFpEaY8wBJ4ONMU8ATwBMmjTp2DrYu2JxN3io0S6bSil1kI4k/eXAMBHJwSb7a4BrO/LixphvNj0WkRuASa0TfqdzxRGLlzK9ZaJSSh3ksOUdY4wPuBl4F9gIvGyMWS8i94vIJQAicrKI5ANXAY+LyPrjGfShSFQs8Y5GPHrLRKWUOkhHWvoYY+YD81tNu6fF4+XYss+hXuNp4OkjjvBIueKIczTqfXKVUqoN4XVFLoArljjx4tHyjlJKHSQMk74bt2hLXyml2hJ+ST/KjZtGbekrpVQbwi/pu9zEaktfKaXaFJ5J33ho8GpLXymlWgvLpB9DIx6ftvSVUqq18Ev6UW5itKWvlFJtCr+k73LjMh48Xh1aWSmlWgvDpB+LkwA+b2OoI1FKqW4nDJN+nP3tqw9tHEop1Q2FX9IPjqnv8DWEOBCllOp+wi/pB1v6oi19pZQ6SBgmfdvSjwp48AeObWh+pZQKN2GY9G1LP7ZlX/1/Xw3r3whhUEop1T2EX9Jvuk8ujbavvt8Hue/CrqUhDkwppUKvQ+Pp9yhNLX0JtvQb6+z0hsoQBqWUUt1DGCZ929KPJXhVrqm20zXpK6VUOCb9VjX9QI2drklfKaXCMOk31fQlWNNHk75SSjUJv6TvcgO2pd/g9Wt5RymlWgi/3jsxiQAkU2vvnuVp0dKvKQavXqmrlIpc4Zf0nS58sb3JkArb0m8MJn1PFfzfMHjx2tDGp5RSIRR+SR/wx2eSIRUHtvSbbF0cmqCUUqobCMukH4jPJF3Kgy396lCHo5RS3UZYJn1J7EOmVODx+g9u6cckhyYopZTqBsIz6Sf1IY1Ke/esxlZJ361JXykVucIy6Ucl98UlfmrK99JYV3XAvJ1Vhn98vC1EkSmlVGiFZdJ3JvUFYMf2PD5at50GiWme56/n1/M2hio0pZQKqbBM+iT0AaCiOJ9ofx0F/t77ZyVSR2Js+F2TppRSHRGeST8xE4B0qSBB6ikyzUk/gXoGpLhDFZlSSoVUh5K+iMwQkc0ikicid7Yxf5qIrBIRn4hc2WL6eBFZKiLrRWSNiHy9M4NvV0IfDEKWlBJPA1XE8al/DHmBfjjFkOzydUkYSinV3Rw26YuIE3gEmAmMBr4hIqNbLbYLuAF4vtX0OuDbxpgxwAzgzyLS61iDPixXLPTO4YzkUtJcHmpx8/Ok3/C0/wI7v3U3TqWUihAdKW5PBvKMMdsARORFYBawoWkBY8yO4LxAyycaY7a0eFwoIsVAOlBxzJEfhmSMZmLpFrxeDzUmloGp8SSZFLsb8ugFW0qpyNSR8k4WsLvF3/nBaUdERCYD0cDWI33uUckYBaVbcDVWUYObjMQYfnbJyTYWvUpXKRWhuuREroj0BZ4FbjTGBNqYP1tEVojIipKSks5504xR+x9ujB7H0IyE/SNwOr1a3lFKRaaOlHcKgP4t/s4OTusQEUkC5gG/MMYsa2sZY8wTwBMAkyZNMh197UPKGLP/4W9/egtxsS7Yay/UivbX4fMHiHKGZ+clpZRqT0ey3nJgmIjkiEg0cA0wtyMvHlz+deBfxphXjz7Mo5A6BBwuOPNOkuNjcDkdEJME2G6btY3+Lg1HKaW6g8O29I0xPhG5GXgXcAJzjDHrReR+YIUxZq6InIxN7inAxSJyX7DHztXANCBVRG4IvuQNxpjVx2NlDuB0wd3FINI8LToBgASpp9bjI9ntOu5hKKVUd9KhS1ONMfOB+a2m3dPi8XJs2af1854DnjvGGI+eo9WBTLCmn0A9dY3aV18pFXkiq6jtchMQJ/HSQI1HyztKqcgTWUlfhIArnkTqqPVoS18pFXkiK+kDgegkEoM1faWUijQRl/RNTBJJ1FGrNX2lVASKuKQv7l4kSa3W9JVSESnikr7DnUQiWt5RSkWmiEv6zmBLv06TvlIqAkVc0hd3L5Ko0/KOUioiRVzSJyaJBKmnut4T6kiUUqrLRV7Sj03GgaGq6rgP6a+UUt1ORCZ9gPqqshAHopRSXS8Ck74dadNTUx7iQJRSqutFYNK3LX1TX4nXf9D9XJRSKqxFbNJPkjpKa/RkrlIqskRe0g/eSCWROoqrNOkrpSJL5CX92F6AbekXV2vSV0pFlghM+raln0QtxdUNIQ5GKaW6VuQlfacL44ojSeq1vKOUijiRl/QBie1FpqteW/pKqYgTkUmf+DT6RNWQX14f6kiUUqpLRWzSz3BWsXtfXagjUUqpLhWhST+dXqaSgop6/AET6miUUqrLRGzSj/dV4PUb9lZpXV8pFTkiM+nHpeLy1+OmQUs8SqmIEplJPz4dgFSpZreezFVKRZAIT/p6MlcpFVkiNOmnATAsrp6dZbUhDkYppbpORCf9kckeNhVVhzgYpZTqOhGa9G15Z2i8h7ziGjw+vUm6UioyRGbSj46HKDcDYmrxBQy5e2tCHZFSSnWJyEz6AMlZZHrzAdiwpyrEwSilVNfoUNIXkRkisllE8kTkzjbmTxORVSLiE5ErW827XkRygz/Xd1bgx2zQ6cQVLiMp2rChUJO+UioyHDbpi4gTeASYCYwGviEio1sttgu4AXi+1XN7A78CTgEmA78SkZRjD7sTDD4baaxmVloRq3dXhDoapZTqEh1p6U8G8owx24wxjcCLwKyWCxhjdhhj1gCt7zR+AbDIGLPPGFMOLAJmdELcxy5nGiCc797A+sJKGrx6MlcpFf46kvSzgN0t/s4PTuuIDj1XRGaLyAoRWVFSUtLBlz5Gcb0hfQQjAtvw+g3rCyu75n2VUiqEusWJXGPME8aYScaYSenp6V33xik59PYWAbByZ3nXva9SSoVIR5J+AdC/xd/ZwWkdcSzPPf5SBhJVuYuc1Dj+tXQn20q066ZSKrx1JOkvB4aJSI6IRAPXAHM7+PrvAueLSErwBO75wWndQ6+B4K3l4Uv6U+vxce9bG0IdkVJKHVeHTfrGGB9wMzZZbwReNsasF5H7ReQSABE5WUTygauAx0VkffC5+4AHsDuO5cD9wWndQ8ogAMbGVzBrfBbLt++j0df6XLRSSoWPqI4sZIyZD8xvNe2eFo+XY0s3bT13DjDnGGI8flIG2t/lO5gy+HSe/mwHa/IrmDSod2jjUkqp46RbnMgNmV7BpF+xk1NybKJftq0shAEppdTxFdlJPyYB4tKgNI+U+GhG9knkix3ai0cpFb4iO+kDDD0X1r8OVXsYm5XM+oJKjNGbpSulwpMm/bPugoAPFtzO+HShorae4mpPqKNSSqnjQpN+7xw471ew8S2++cEZPOh6XK/OVUqFLU36AKfeApc/CcBY2c76Ah11UykVnjTpNxl3NZzyffo7ylibr6NuKqXCkyb9lnoNwE0D67du11E3lVJhSZN+S8GLtVK9RXySWxriYJRSqvNp0m8peLHW8Jgy3l5TGOJglFKq82nSb6nXAAAuyGrkk682sTFva4gDUkqpzqVJv6XYJHCnMC29jkdiH8X74re1tq+UCiua9FtLGURMxVYmOrcy3LuZe99YHeqIlFKq02jSby1rIuz8lChfLbHiZfWqL3QQNqVU2NCk39rAU8E0j6k/LSGfX76xTsfZV0qFBU36rQ041f52uMAVz7cGlpNbXMM9b65jq95OUSnVw2nSby2pL/QeDBkjIWsC/Us/4apxqby4fDeXP/oZVQ1eu9zcH8FDo0Mbq1JKHSFN+m25+GGY+SBMuw0qdvJg2nxe/d5UKuu9PP5hsBvnqmegqgCahmGuKYEdn4YuZqWU6gBN+m3JOcPW9gefBSMvQta+wqRBvbn4xH48smQrv3l1afOyDcEROT97GJ69DAJa+1dKdV8dukduRBt0Bmx6Gxbfz5+jdjB08rdZvWIxRAfn5y60ib9iJ/g9UL8P4tNCGrJSSrVHk/7hZE20vz/+I07g1oRPKB4wEors5LqFDxBXWwCZY+yE2pLDJ/1dy+D5r8N3P2q+ObtSSnUBLe8cTp+x4AjuG8+8AwJ+Moo+pGbAOQDE1ewC48fsXWeXqS1pfm5jLez+AjbNg98NhL9OgvoKyFsMDRWw8a0uXhmlVKTTpH84rljIPAGi3HDarfCd9+G/FpJw9eMHLCbBvv3+zx6BR6dCwA8f/xGemg6vf9/uOMpybTmocJV90pZ3unptlFIRTss7HXHmz2wLPjre/qQMtEkdAVrdRD13IRAgULUHR95iO81TCdf9xyb/zfOh8EsQB+z8DD74HZz6I4iO6+KVUkpFIm3pd8TIr8HEGw6c5nBCXOpBizqxLf4HnnwBs+crzNSb4YZ5MPQ8GDED1r8OdWVw8ncgdQh88L8w95bmrp9KddTaV2H9G6GOQvUwmvSPRUKG/S0H/xunN7yDYLhpeX/O+Y+PheuLqBtzbfMC478BNy+Hc34J6161OwOljsQnf4alfwt1FOpI7V0P5TtD9vaa9I9FfLr93W+C/d17yP5ZUwNf4nXGETdoIg4RZj+7ktFP7mNWzD9YOO4hGtLG2gVP/zFkjIbF90PVni5eAdWj1RZDTXGoo4gcNSXw5Lmw+AH4/HGoDQ7E6PfCc1dA7iJ7xP7qTfDR/7V/9P7CNTD/9q6LuxWt6R+L+HRA4NSbYfM7ULMX9tkrdsX4cWVP4G/XnUyD188Hm4vZVlrLx1tKmf1FGWkblnDFhGyumJjN8PMfgOeuhD+Ngcseh3FXQVUhJPUL7fqp7ivgh9pSiIoJdSRHr74CXHEQFX34ZTtL8Sb47K9wwuW2F92599jOGh3xxeNQsML+AOz+HK6cAzs+gbz3wBkDfcfbI3ew6zb1B7DnK9ugGzgVGuugYpfdUYSItvSPxZBzYPQlMOYyuPzx5pZ/VPBD1G88ALEuJzNO6MsPzhrKC7On8NLsKYzv34unPtnO+X/6iP/NzeKds97El30KvPE9WPUveGgULHssRCumur26fWD84K0DTzcfCNAY+PI5m3BbTnv8DPjgt10by9pXYPVz8NzlsOwRyH23eZ6nBhbdA09dYDtbNPE2wMcPwRdPwMiL4JZVtvPFuv/Yzhgb59rltn0ARV81P2/LAvDWw9MXwQtfhwV3Qv4Xdl71HijNa76ivwtpS/9YnPRN+9OkKekPmGI/AP1OavNppwxO5ZTBqZTWePjNvI08/uE2AHLiZ7NIVuF4+yd2b7zoHlj+D9v6z5oIIsd1dVQPUtuirFNbAjEJoYsF7DUpHz8EJ/+3HbSwpW0fwJs/hJhk+NbrkD3RxlyxC3YvP/Tr5q8EZxT0PbFz4txnv2uMutgm7A1vQnwGvHMnuNy29R6TZIdUuXGBvZYmdyHkL7dl2LN/YTtgnHWnTfrzfwY1RfY1aoth5dP29UdeBFuX2N56nipI7AvbP4L4Fp0/Hp9mrwP6r3e69LvdoZa+iMwQkc0ikicid7YxP0ZEXgrO/1xEBgWnu0TkGRFZKyIbReSuzg2/mxl7BZxxm92Q0G7Sb5KWEMNDV5/I4p+eySvfm0pqWiaveqbgCHj50nkiFX2mQnURLLwbfjfAfkgKV8M/zoMNc7tghVTIvP3jQ2/jmr3Nj1teENgRhV/aBoXPc3SxtWXBz+Dj/4NP/3LgdGPg/V9DUha4e8GL10JpLhRvtPNLNrb/mmVb4R/n2M/70WiotEdEYMuvj0yx18gMnwFff84m/s3vwCvXQ9Fa2LUUpj8As5fY/82cC2DJb+zO6fIn4QdLITM4sm50PEy/H/autTu8Sx+z5Z2Nb0F0oj3699bCe/faHcJpt0JVvt3JpOTY1/DWwu5ldvDG8p12fbvgHM1hW/oi4gQeAaYD+cByEZlrjNnQYrGbgHJjzFARuQb4PfB14CogxhgzVkTigA0i8oIxZkdnr0i3kDXR/hSssh+apo17CCLCkPQEhqTDK9+bypbVtxGY+wn/irqUN7aN4Jk+rzJt12v4ouJw7tuGPHe57fI57yf2KGDyd2DNy3DSt2D4+YePceHdttUx9YedsMLHWSBgy10TrodBp4U6mq5TXwEr5tia/ehL2l6mpkWiP9JE8e7dsPMTm2iufubwyzdUQv4KOwhhXRkkZ0P1Xnj2Urj6X+BvtOWb6ARbPjn/AZsI3b2gcretgV/wW8g5016s+LdJkBlsGNWV2R3A8qfsEfLYK5vfd+6P7G9/I3iqISbxwLhKttidX84ZB8f8yZ9swnW4bKu8LK95BzP6Uvt7wvWQ9z44HPaiS3HYBpuI/X589AcY/0249NG2/y8nXAGNNTBgKqSPsDuRda9CyiDof4pdpmIXTL0ZBp3e/Pf5v4FP/wwBn33Pt26FpGyIS7G1/u8vtTEdJx0p70wG8owx2wBE5EVgFtAy6c8C7g0+fhX4m4g0XbkULyJRgBtoBKo6J/RuLGuC/TlCIsKIk86A0Tt4ADfpi3P507JTOF1e56H6i0h31XMjb+GJTSe6bh+y/UN7Esn47WHr4ZL+pnn2JFZSFkz5gR0aes3LkD4SRl54lCt7HFXlw5qX7AmxY0n6ZVvtFyx9ROfF1tkCfnvtB0DRGvu7LK/95Q8o7xxB0vfWQ8FK+3jDG3bnkZDePD93kS3F3LjAljF2fwH/vvLA2vON70B9ORRvsKUbb52dPv3+YGPkKVh8H5x8E2RNsvMGTIU+J8APP4dXbmiOAeDRKfb3Vy9Acn/oOw5Kt9gd05BzYOv78OGDNrZeA+CbL9vl5//UniT92Y7mJLn0EVt7r9gNwy4Apwvef8CWlpqkj7S/sybAj9e2/X867X9smWfCt9r/X4oceP3OSd+0Sb++HHr1t0cUydlw7q/sto1NBneKbag5ouxOMXWoHYtr4d32837N88c14UPHkn4WsLvF3/nAKe0tY4zxiUglkIrdAcwC9gBxwI+NMftav4GIzAZmAwwYMOAIVyEMxSSSAPz8wlE0TB/OuvWTmBrXn3c/+QLPzgX8vnoGnzpP5ntjHVy2/mb7nMbag19n/Rv2S5Q9ERqqYN5ttuVTVWC/SG9837aUEvt1LOkbc3Dtsb7CfmhHzGj/eX4fbP/QfvBzzrSvEfDbE9bjrraHyq01VNkvPtjk0qQ0DxIzbatv41v2aCp3of3y5L5r1/esFhXIhkr46wQQJ/zqoI9e91C+05Ywpt0Gp3zXJjII7qyCO4PPn7CJZMRMeyS57UNwRttWcG2pXb62DHz1NtG0Z+sSu8ypt9gGQPEGSDjTxrDu1ebW81u3wtefhf/cBLG97P0lchfaOvaGN5sHCizZZONMHwkTvm3r+u/eZW85+tlf7ZGvM9oOZQI2aU+8wSb9lEFQvsNOn/kHWPgLmHO+7fqcOsQOfXLRn+Ev4+zQ5THJtrW+a5ndge/41DZ4SjbaAQ+ri2wpKTrexnP5E/aIe/N8e1W8wwUBr71B0uHEJMBpPzqy7ZhzFoy53B4dAFz70oHzr3rGXtsTFQNTvtc8PTu4Y/R77IWgx9nxPpE7GfAD/YAU4GMRea/pqKGJMeYJ4AmASZMm6aWpLcS6nIwbbz8UZwy/mOLCLzmvLpaiz3fz45VFvO64g//ut5NppS/yvScWMXn0UK6bMpC7X1vNgxuuty/y34th2aO2x8AV/7Bf5BeusSeuxlxmLwxrqLRfsl2fgbu3bW018XvhjR/YBPHdj21LpKbY1kDzV9gv5LUvw/ALDgzeWw+r/w1bFjb3krj2ZVtndbnh7f8BXwNM+f6Bz6vMh0dOsUkBYO8Gm1ji0+DJs+37zHwQXrnRftmL1tq7nZVvtzuDM35qW3hgr38AmxxatqYba6Fks91ZxCZ1zsY6GgG/bfnWFsO61w5XMxIcAAAYcElEQVRM+n6PLY+4U+Ddn9v1H3y2rYtX74GETJvUqgrt//qfM+3Q3ressuM87dtuSxBNO+r1b9gdfXwGTP5uc9IffCa89yv7ORCnfd0dH8PfJtvXu3EB9J8MJ15j/2+b58OIYCOhcDXsXQcTb7T/89N+ZOv7w2fYHULBSpv4W3bLHHUxzPspDDkXVjxl33Ny8Ar14o02ce/bak+athyF9qaFdh3f/KHdzsZvp3/+uP28blloP6s3LbSfhyaDzrCNjrPuhC3vQvqo47MtHQ646p/tzx9ydvvzTr258+NpR0eSfgHQv8Xf2cFpbS2THyzlJANlwLXAO8YYL1AsIp8Ck4BtqKOS0W8AGcCpQzNYvbuCN1cP4rGl85gWDfU7lvPrbQ1sWPYOu8tqoakL9z/Otb/PussmgcX32VbRNc/bfsPrX7dXCS57zHY/c/eG27Y0J84Pfw9rg4fURWtsV9SFv4Q1L9o6LtjeD4NOP7DV/vFD8NGDtm553r12nKH5t9t7DzQdbm+ad3DS/+yvtlbaNHJpY7VtrQ841faEWPca9BpoW217VttlynLt7/py+wUfep7deax8xnah9TXYElivgTaRPX2RfU5Kjk2S1Xts7O5enbOhDsXvsz1SwLacC1fZ3in5X9hy2/aPIS4N6kphxT9tyzDgtTG+c4f9DbZFHhULK/9py2BNZZYFd9idbF2ZbZ1f+Ad7RDb3FsgYBVf+0x4NxKXapL9vm229g02k59xtPw+f/92eoOw/uTn2ETPtQIFNgwU29VkfFjzZetK3bEPgtB/ZHcLcm5uHJ2/iToEb5ttW/6T/gsQ+dsc09Fz7M+Qc+5lpapHf+I49oskYCZf81X6OCr+0veXq9tkToU2+9tCBCR9sfd7XYLtZTrvt6LdbmBBzmDFfgkl8C3AuNrkvB641xqxvscwPgbHGmO8FT+Reboy5WkTuAEYaY24Ukfjgc68xxqxp7/0mTZpkVqxYccwrFkm+zN3J+H+fCA4nfgNRxke9Iw53oI7v+O6gt6OGHc6BVPUazfmjMznNsY7s3nFknngBzpoieGgkjL3KnoQbeBrs/BS+9kfbs6F+n23Vu1Nswj/vPjjxG/ZCskDwApOJN9quaidcDlc8Zb/ANcXwl/H2SzzrEduafvZy2Lq4OXBx2iRzwpUwepZtAdaWwp/H2i+58dt6flMy2/88hy0fuHvb+JoO253R9mfUxbbF/N59tsV6+ZPw6o32XEZ0gm0llmyy77n63zB8pu1THZ8BP17X+Rc8eartkdKZd9gL7v4yHs67Byb+Fzxysk3cF/3JnuQEu0M85xe2xdy80vYkY9Eau77DptuSSf5y2LbEtmYTgmWvlf+0/7cJ19sLihwuwNg68vc/bU6KT19kS30xifaoZ/Jsu+O/9SsbZ81em5BbqiyAP7W6N7TDBb8oat6RNfF7bY1/4o1HdY7rkIo32s/PvJ/YbXzpY3adx1zaue/Tg4jISmPMpMMtd9iWfrBGfzPwLuAE5hhj1ovI/cAKY8xc4CngWRHJA/YB1wSf/gjwTxFZjx2S8p+HSvjq6Jw0bCDM+huUbiEKgeKNuHPfpU7iaBh4NrEZiQwJGNYVVPLw+7n8xcQAfmJfe4f0hGg+Blj7CvWuFIrPe5yBz51mD7+bWshgD7U/etCWAZb81p4YPfce263wgt/aevPi+22JyN9oW5n+RnsSq6l8MuQcm/SzJ9tW7Rk/ta+57lX7M+1nNtH7Guz7Lfm1bbE3XfwCtt47/X748llbbvjkTzDwdPjqeZsEM0bD8idtvd8E7CH90OCRTlWLA9QrnrIt2NX/tgk/ub8tpeQusrfJrC0+sMVYvtP2ljr1luYxlzpq3Wt2HWpL7WG8pxIW/crWysvy4LInbGt40BnQZ5xdP4ezOen3O8mWU8ZdDf+YDgNOsV0OIXiRlmnu/22MPfkoTntENu5qezQhDjjpugPXKWOUTZgOlz3qGzbdHnU1XQneOuEDJGdB2nB7vqXXQHvUlnPGwQkf7JHiJX89sv9VR2UESzRX/MM2MFqWI9UhHbal39W0pd8JitbB30+zSeSGtw+YVdfoI6+4hk1F1WwuqmZvVQNXbvoJZzm+5JrGu1kWGM0fkl5muiwnf+bTjPj8LlzFa+D2PNuDYunf4MRrYfy1B3aVM8b2d97wZnNL/NxfwRk/aV6mYje8dJ29dB1sAtq7zpZY5t9uE7cz2pYQvvYne1XyeffahBiXarv6jf8mXPrIge8rEiyJpNpE9fB4G8N3P7Y7I4A/joLqQnvxkLs3nP1z+7y/TrJlnmtftrXixL42Odfvgx9vsMnUGNuiXDHHHg3ctBB6H6Y7biBgj5xGXWyv/sxfYY9GMsfaowyHM3iOIcr+b9u6uKpgpT06aZl8PTV23TpjKO6idfYIbdJ/Nfc/74j5P7NHEOfeY7fpmXccfEGW6nIdbelr0g9Xc39kSzUnfv2wi+4uyKeipp5e6Vm8t3Evi9btYdmOfQSMMNhVzq0To1nOKCorK7h2uDB16ukYY5DWPXm8DfZik7QRtq4+9qrmE6eH01hnT+A1VNodReoQeyIyqV9zuWXDXNuiazrB256itfZCmfThzdOeu9KWMG796sAucUt+a09w/mCpvWBp6d/sib6SjXDh/9mjnY//aM8lxPaydfakLHtklTXR9nZZ+AubmHd/YVvkZ91luxs+e5n9X5RutkcuXz5nW8b9p9hW9fsP2N4ehzr51x1tXmA7AnzjpUP32lJdSpO+OiZFlQ1s2FPJX97L5av8SmJdDpLdLvZWechOcVNa4+HCE/ryy4tGU+/1EzCG7JRufCOYit223JQ65OB5TUcLPo8tFSRn27ufidiTwZ7gpSVXzrHnNl6+wU678in49GF7MjTgt2WYghX26MTbAB/+zj4vdajtQbXuVVs2O/3HcNbPbflm4vWHvXK72wkEYOObMPLitss6KiQ06atO4Q8Y9lTWk54Yg1OEv3+4lU/zyujf280bXxbiCwQIBD9CkwamMLJvIgN6xzE0I4Ezh2fgdPTQ8YKWPWZ7JEXFwuk/sWWrmxbaMoynGp65uHlQrquehlGz7BHE81+3Y7r0GmC7N46eZful986xO4J377IXxqUNC+XaqTCkSV8dd6t3VzBvTSGD0uKpqPPyzroidu2ro7Le9uoZkZlITlo8e6oauHZyf0b3TWZMvyQcwR1Bg9eP1x8gMdYVytVoX1WhrZ+3dUKzqtD2Dx82vfkSe7AjST46BTC2N8yFf+iycFVk06SvQqay3suSTcX8a+kOyuu8OB1CXrEd/jc7xW27ZKcn8EleKV6/4VtTBlJZ7+W/z8hhXHYX9JM/3t671/YquuKpA8eSUeo40qSvug1/wPDlrnK2ldayYO0eopwO1uRXcM7ITGo9PuZ+VQhAanw0s8ZnMaC3G1eUg9376omOcvBpXinnj85kxgl96BUXTbK7mx4ZNAkEYNv79urZjp7IVuoYadJXPYIxhk/ySkl2u/jpy19RUFFPXaO9vD7KIfgChozEGIqrm4cBzurlJsbl4IR+yVwwpg/7aj0UVDSQEONk1746+iTFMvvMIQSMYeH6vfTrFcupQ9JCtYpKdQlN+qpHMsZQUu0hYCA9MYZGX4BYl4MF64qoafBRUuNhU1E1jT4/n+WVUe3xAeByCl6/ITEmimqPj8Fp8Xh8AQoq6nG7nNx7yWhcTgcTB6bg9RuyU9w0+gNUN/jI6uUO8Vordew06auw1+D1k1dcQ7LbRXaKm3qvH5fTwYod5Xz/3yuJcjh4YNYYfvafNVQ3+A54brLbhcsp1DX6+d/Lx9InKZax2cnMW7OHkhoPM8b0YXtpLXsqG7h28oD9J5+V6q406auIVlbjwSFCSnw0K3bso6iqgdT4GHbtqyXW5WTu6kLKahspqfZQUFEPQFJsFFWtdg4A543KYHB6AgN6x7FrXx2TBqaQleJmbX4lM0/oS3JcNz/HoCKCJn2lOiC/vI71hVWsK6hk5c5ybjlnGOmJ0SzfUU7/lDg+317G4x9tAwON/gAOYf91CQC94lzMGNOHPsmxnD0igwXrirjxtEFkJsWGbqVURNKkr1QnMcbg8QUorKgnK8XNyh3lbC2tZWh6As8t28lHuSXUeHw0fZWyerm55uT+5JXU4HY5qWv0U9fo42vj+jLrxCwtFanjQpO+Ul1oc1E1L3yxiymDe/OHdzeztaSWzKQY/AFDfEwU/oAhv7yeU4ekcuXEbJwOISMxlpy0eDKTYqhq8LFqVzlulxOX08Hovkns3FdLsttF32Q90awOT5O+UiFUXttIrzjX/kHpAgHDyyt288DbG6gNdkltckJWEnnFNTR4A/unDegdR0FFPQ6BKyf255Sc3gxJT2BU30Se+mQ78TFReoJZHUCTvlLdUIPXT0FFPYGAoaiqgfWFVby2Kp/RfZO4+uT++AOGvVUe7pu7nlH9khiemcDLy/Np9NsdQkyUA4/PPp42PJ3rThnA9tJa1hdWMTwzgYCBHaW1/OaysTz58TbK6xq5c+ZI6jx+UuKjDxWa6uE06SvVg9V4fLhdTpwOoazGQ3ldIyt2lLN5bzWTBvZmX10jv357w/4dQOueR/HRzv1HFKnx0Xh8AW45Z+j+Lq4XnNCH7BQ3qfExVDV4+XJXBfHRTqYOSUVEWLK5mBc+38WPzh3Gsm1lfPOUgbij9eri7kyTvlJhrqCinqLKeoamJxIX4+TbT31BeV0jM07ow9r8Sr596iBe/GIXS7eVER8dRUFFPWkJ0VQ1+Gj0Bdp8zdF9k0hyR/H59n20TA3nj87kvllj6JMUS7XHR1Ibg+TVeHz8Zt4GZo3P4oSsZH63YCPnjsrk7BFHeKcxdVQ06SsVYQIBg98YXE7HAdMa/QGqGrxsLa5lyuDeVNZ7WbGjnOJqD6U1HpwOYcrg3mzZW8MrK3bj9RumDU9jTL9k/vp+HicPSuFfS3cCtotqRZ2XG04dxJh+SZTUeCivbSQ1IYYtRdW89mUBLqfQN9nNrn323sY/v3Aks6e1cR8D1ak06SulOs2WvdV8tKWEDYVV+AJm/yB5cOB5hisnZhMT5WB9YRXXnzqQ9zYWM2/NHn46fTjuaCeN/gC799VTWd/IjBP6MmVwbzbuqWZQahwDU+PbfG9f8HxGVIudmTpYp90YXSmlhmcmMjwzEbDXLfzo3KG4nA7SE2OIi45iX20jX+4q57ShacS6mmv/F4/rh1OEPy7asn9aUmwU8TFRzF9btH+aiO2x5HY5cUc7GZ6RSEZSDFv2VrN0axkAQzISiHII2SlxDOgdh9MhlNZ4mJzTm2EZiQzPTDigt1S91098TPspzhjDrn117e5swpW29JVSx5XPH+Cfn+5gXHYyo/slERcdhTGGl1bspsEbYGSfRJbv2MeO0lrqvX5qPX5W766g3utnYGocEwakBHs1NdDoC1BU1UBhRT0BA7Eux/6urm6Xk97x0Zw9Mp0vd1Wwo7SW/zlvOHsqG0hLjOY7ZwzG5XSwraSGJZtLEOD+tzfwwnemMHVIamj/SZ1AyztKqR6r0RfAYIiJarvHUGmNB68/QEpcNNtKalm9u4Lc4mq2l9aycmc5KXHRxEQ5yC22V0XXe/2cNKAXCTFRrNhRTr3Xv39IjSHp8Yzvn0Ky28U1k/szKDUel1MQEfwBw5JNxQSM4awRGTT6AyQc4ughlLS8o5TqsaKjDl2/T0uI2f94dL8kRvdLOmiZ+kY/W0tqGNU3iec/38njH20jEDB8bVxfPL4Ab31VyNcn9eelFbupavBRVe9lzqfbAXvUcMXELDYXVbN8RzkA47KTWV9Yxc1nD+XH04fj9QcOOGkOUOvxERft3F9mMsbsf9xdaEtfKRVxvP4Am4uqOSErmYq6RnrFRVNW4+HtNXuobvCSW1zD3K8KSUuI4bbzh1NR5+V/F2wiPTGGkmoP2SluCirqmZKTisfnZ0SfJE4dksrPX1vL6H5JPHjlOJZuLePhxblMGtQbh9jbiI7pl8zlE7LoHR9NQUU9QzMS2j2aOVJa3lFKqWPQ4PUTE+XY31LPK64hO8XNEx9tY8vealLjo3lrzR76JMWyrdQOo9EnKZaK+sb95xmGZSSQW1xDTJSDgalxbC2pJSUumoAx7KttZERmIv/+zim8/VUh9d4AN52ec9ijnPZo0ldKqS5SWedlwbo9nD4sDYcIr39ZwNCMBM4fncnn2/eRmWQH19uyt5rLHvmUuJgobj57KL+dvxF/wOALjtc9vn8vXvv+qUc1ppImfaWU6oa2l9YSF+0kMymWtfmVvLG6gJy0ePomx7KvtpGrJvU/qtfVE7lKKdUN5aQ1XxcwNjuZsdnJXfr+eombUkpFEE36SikVQTqU9EVkhohsFpE8EbmzjfkxIvJScP7nIjKoxbxxIrJURNaLyFoR0ZuHKqVUiBw26YuIE3gEmAmMBr4hIqNbLXYTUG6MGQr8Cfh98LlRwHPA94wxY4CzAG+nRa+UUuqIdKSlPxnIM8ZsM8Y0Ai8Cs1otMwt4Jvj4VeBcsZ1bzwfWGGO+AjDGlBlj/CillAqJjiT9LGB3i7/zg9PaXMYY4wMqgVRgOGBE5F0RWSUiPzv2kJVSSh2t491lMwo4HTgZqAMWB/uSLm65kIjMBmYDDBgw4DiHpJRSkasjLf0CoOXVAtnBaW0uE6zjJwNl2KOCj4wxpcaYOmA+MKH1GxhjnjDGTDLGTEpPTz/ytVBKKdUhHWnpLweGiUgONrlfA1zbapm5wPXAUuBK4H1jjBGRd4GfiUgc0AiciT3R266VK1eWisjOI1uNA6QBpcfw/O4kXNYlXNYDdF26K10XGNiRhQ6b9I0xPhG5GXgXcAJzjDHrReR+YIUxZi7wFPCsiOQB+7A7Bowx5SLyEHbHYYD5xph5h3m/Y2rqi8iKjlyK3BOEy7qEy3qArkt3pevScR2q6Rtj5mNLMy2n3dPicQNwVTvPfQ7bbVMppVSI6RW5SikVQcIx6T8R6gA6UbisS7isB+i6dFe6Lh3U7YZWVkopdfyEY0tfKaVUO8Im6R9uULjuTkR2BAekWy0iK4LTeovIIhHJDf5OCXWcbRGROSJSLCLrWkxrM3axHg5upzUictB1G6HUzrrcKyIFwW2zWkQubDHvruC6bBaRC0ITddtEpL+ILBGRDcEBD28NTu9R2+YQ69HjtouIxIrIFyLyVXBd7gtOzwkOVpkXHLwyOji93cEsj5oxpsf/YLuSbgUGA9HAV8DoUMd1hOuwA0hrNe1B4M7g4zuB34c6znZin4a96G7d4WIHLgQWAAJMAT4PdfwdWJd7gdvaWHZ08LMWA+QEP4POUK9Di/j6AhOCjxOBLcGYe9S2OcR69LjtEvzfJgQfu4DPg//rl4FrgtP/Dnw/+PgHwN+Dj68BXjrWGMKlpd+RQeF6opYD2T0DXBrCWNpljPkIe31GS+3FPgv4l7GWAb1EpG/XRHp47axLe2YBLxpjPMaY7UAe9rPYLRhj9hhjVgUfVwMbseNk9ahtc4j1aE+33S7B/21N8E9X8McA52AHq4SDt0lbg1ketXBJ+h0ZFK67M8BCEVkZHIsIINMYsyf4uAjIDE1oR6W92Hvqtro5WPKY06LM1mPWJVgWOAnbsuyx26bVekAP3C4i4hSR1UAxsAh7JFJh7GCVcGC87Q1medTCJemHg9ONMROw9y34oYhMaznT2OO7HtnVqifHHvQYMAQYD+wB/hjacI6MiCQA/wH+xxhT1XJeT9o2baxHj9wuxhi/MWY8dhyzycDIrnz/cEn6HRkUrlszxhQEfxcDr2M/DHubDq+Dv4tDF+ERay/2HretjDF7g1/UAPAkzaWCbr8uIuLCJsp/G2NeC07ucdumrfXoydsFwBhTASwBpmJLaU0jJLSMt73BLI9auCT9/YPCBc96X4MdBK5HEJF4EUlseoy9+cw6mgeyI/j7zdBEeFTai30u8O1gT5EpQGWLUkO31KqufRl224Bdl2uCPSxygGHAF10dX3uCtd+ngI3GmIdazOpR26a99eiJ20VE0kWkV/CxG5iOPUexBDtYJRy8TZq21f7BLI8piFCfze6sH2zPgy3Y+tgvQh3PEcY+GNvb4CtgfVP82NrdYiAXeA/oHepY24n/BezhtRdbj7ypvdixvRceCW6ntcCkUMffgXV5NhjrmuCXsG+L5X8RXJfNwMxQx99qXU7Hlm7WAKuDPxf2tG1ziPXocdsFGAd8GYx5HXBPcPpg7I4pD3gFiAlOjw3+nRecP/hYY9ArcpVSKoKES3lHKaVUB2jSV0qpCKJJXymlIogmfaWUiiCa9JVSKoJo0ldKqQiiSV8ppSKIJn2llIog/w/UnIZ/ckb20gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.savefig(\"30f_future.png\")\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"30f_future-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "[1900.  953. 1919. 1079.]\n",
      "[0. 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "(173, 124)\n",
      "(173, 120) 173 (173, 4)\n",
      "(173, 30, 4) (173, 4)\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "values = read_csv('bb-cross-test1.csv', header=0).values\n",
    "print(len(values))\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "\n",
    "scaled = scaler.transform(values)\n",
    "print(scaler.data_max_)\n",
    "print(scaler.data_min_)\n",
    "\n",
    "# specify the number of lag hours\n",
    "#n_seq = 15\n",
    "#n_features = 4\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_seq, 1)\n",
    "print(reframed.shape)\n",
    "values = reframed.values\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_seq * n_features\n",
    "test_X, test_y = values[:, :n_obs], values[:, n_obs:n_obs+n_features]\n",
    "print(test_X.shape, len(test_X), test_y.shape)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "test_X = test_X.reshape((test_X.shape[0], n_seq, n_features))\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-058328fc231d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"30f_future-model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# invert scaling for forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "K.clear_session()  # Clear previous models from memory.    \n",
    "model = load_model(\"30f_future-model.h5\")\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = scaler.inverse_transform(yhat)\n",
    "\n",
    "# invert scaling for actual\n",
    "inv_y = scaler.inverse_transform(test_y)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()  # Clear previous models from memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 30, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3368421 , 0.70094436, 0.34601355, 0.67191845],\n",
       "       [0.33631578, 0.6977964 , 0.34549242, 0.67099166],\n",
       "       [0.33631578, 0.6956978 , 0.34549242, 0.67006487],\n",
       "       [0.33578947, 0.6935991 , 0.34497133, 0.6691381 ],\n",
       "       [0.33578947, 0.6925498 , 0.34445024, 0.6691381 ],\n",
       "       [0.33526313, 0.6915005 , 0.3439291 , 0.6691381 ],\n",
       "       [0.33526313, 0.6915005 , 0.3439291 , 0.6691381 ],\n",
       "       [0.33473682, 0.6956978 , 0.34340802, 0.6691381 ],\n",
       "       [0.33473682, 0.699895  , 0.34340802, 0.67099166],\n",
       "       [0.33421052, 0.703043  , 0.3428869 , 0.67191845],\n",
       "       [0.33421052, 0.70619094, 0.3428869 , 0.67284524],\n",
       "       [0.3336842 , 0.70828956, 0.3423658 , 0.67469877],\n",
       "       [0.3336842 , 0.7093389 , 0.3418447 , 0.67562556],\n",
       "       [0.33315787, 0.7103882 , 0.34132358, 0.67840594],\n",
       "       [0.33263156, 0.7114375 , 0.3408025 , 0.68118626],\n",
       "       [0.33263156, 0.7114375 , 0.3408025 , 0.68211305],\n",
       "       [0.33263156, 0.7114375 , 0.3408025 , 0.68211305],\n",
       "       [0.33263156, 0.7114375 , 0.3408025 , 0.68211305],\n",
       "       [0.33315787, 0.7114375 , 0.34132358, 0.68211305],\n",
       "       [0.33315787, 0.7114375 , 0.3408025 , 0.68396664],\n",
       "       [0.3336842 , 0.7103882 , 0.34132358, 0.68396664],\n",
       "       [0.3336842 , 0.7103882 , 0.34132358, 0.68396664],\n",
       "       [0.3336842 , 0.7103882 , 0.34132358, 0.68396664],\n",
       "       [0.33421052, 0.7093389 , 0.3418447 , 0.68396664],\n",
       "       [0.33421052, 0.7093389 , 0.3418447 , 0.68396664],\n",
       "       [0.33421052, 0.7093389 , 0.3428869 , 0.68396664],\n",
       "       [0.33473682, 0.70828956, 0.34445024, 0.68303984],\n",
       "       [0.33473682, 0.70828956, 0.34497133, 0.68303984],\n",
       "       [0.33473682, 0.7072403 , 0.34601355, 0.68303984],\n",
       "       [0.33526313, 0.7072403 , 0.34653464, 0.68303984]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "yhat =  model.predict(np.expand_dims(test_X[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_yhat = scaler.inverse_transform(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[628.5579 , 675.93713, 668.5027 , 737.423  ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e79b1c8ae02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "del history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8de133b6d5dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreset_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-4bb6bae41816>\u001b[0m in \u001b[0;36mreset_keras\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# if it's done something you should see a number being outputted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# use the same config as you used to create the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
